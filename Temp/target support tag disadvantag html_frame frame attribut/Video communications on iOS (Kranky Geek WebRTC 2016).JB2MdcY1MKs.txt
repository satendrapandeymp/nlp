so to continue on the iOS topic happy to invite up another one of our gracious sponsors from Twilio Chris Eagleson so Chris is a Nellore really deep expert in this he's been doing mobile iOS development since you know the App Store was introduced he's been part of a Weber C startup and now he's a he's what I do so I'm really excited to hear he's gonna follow on an iOS and tell us even more Thanks awesome thanks Chad so my talk is video communications on iOS and I'm going to dig in a little bit deeper into what it means to use WebRTC with iOS and talk about a few things that we've discovered at Twilio doing this so iOS at Twilio is is a number of products but today obviously I'm going to focus on the video SDK we also have a voice SDK is not based on WebRTC at the moment but our video SDK is based on the chromium implementation of WebRTC so we take a little bit of a different approach than Eric does in our case we just consume the the C++ SDK and use those interfaces as opposed to building on top of the objective-c api's that exists today so with that said our agenda today is going to be three things we're going to start with audio and I'm going to talk about the audio pipeline and also relate a few experiences we had with with call kit after that we'll get into video looking at the video pipeline and also the included capture that comes with WebRTC and allows you to use the camera after that I think were gonna get into the most interesting part of the talk and that is looking at custom captures and how you could build those to satisfy your own use cases and then I'm going to specifically get into building a screen capture with you guys during this talk so the WebRTC audio pipeline starts with something that's that's pretty important and that's the audio device module this is the central place that is responsible for playback and recording of audio and as Eric mentioned during the Q&A session there's actually several different implementations of the audio device module you actually implement a protocol called audio device generic and you provide for different platforms such as iOS Android Linux Windows and so on and so forth when it comes to codecs there are a couple pretty popular ones that you're going to see when you're using WebRTC and those are Isaac and opus both these are software based implementations on the iOS platform and Isaac is a lower complexity codec it works better at lower bit rates and is optimized mostly for voice opus is more suitable for a wide range of audio content including not just voice but potentially live music or anything else you want to throw at it it tends to do a pretty good job the disadvantage with opus however is that since it's a software implementation it does use more CPU and consume more resources on your mobile device so that's something to keep in mind but at this point opus is the default unless you're going to go ahead and and change SDPs like you saw in the Android talk you're gonna get opus by just negotiating a peer connection so let's move a little bit into iOS and talk about the audio pipeline there so the iOS implementation of audio device generic is called audio device iOS and this class is written in objective-c plus plus so it uses a couple of Apple api's these are av audio session also core audio and audio units so by taking advantage of those last two frameworks that I mentioned we were able to achieve real-time audio playback in recording so on iOS specifically we're able to take advantage of a couple of things that are kind of unique to the platform every single iOS device that ships has a harbor unit which can do echo cancellation and gain so rather than using a software implementation of these features we're going to just defer to the hardware and take advantage of that at all times another class to be aware of and this is one that's in the public objective-c API is that you might consume as a developer is RTC audio session so this is just a wrapper on AV audio session which is Apple's way to manage audio on your device so these are both Singleton's there's only one of them in your application and WebRTC is going to go ahead and manage that mostly itself but I'll get into that a little more later in this talk so next I want to talk about capturing and encoding on iOS so something that's interesting here is that audio capture on iOS runs on this real time thread so you can see in this diagram that the core audio thread over here on the left is responsible for real-time audio capture so this is running and this is a thread that you actually can't create yourself it's going to only be created by invoking core audio and the audio unit api's and so WebRTC does this internally which causes this real-time audio thread to be created and it's going to go ahead and capture audio so what you're going to see while this is happening is the voice processing unit and this is the one that's doing echo cancellation and gain for you is going to go ahead and produce audio and we're going to pull samples out of that unit on the real-time audio thread now there's a little bit of an impedance mismatch between what WebRTC wants as a system and what iOS is going to give you so in iOS we're going to go ahead and set up this AV audio session and ask it for buffers that are approximately 10 milliseconds long but in reality it's going to give us something that's a little bit different than that and WebRTC would prefer or actually must operate in 10 millisecond intervals so in order to actually capture this audio and pass it off to WebRTC for encoding we're going to need to do a little bit of buffering we're just gonna need to compensate for the difference between the 10 millisecond or approximately 10 milliseconds that we're gonna get from iOS and the 10 millisecond sizes that WebRTC actually wants so once we do that we can fill our intermediate buffer and finally when we have 10 milliseconds of audio ready we can deliver that over to WebRTC at which point the WebRTC thread is going to go ahead and encode this audio for us so we can send it to other people so that was a basic description of the audio capture pipeline on iOS at this point I'd like to talk a little bit about what we've learned from using call kit so we recently shipped a second beta of our video SDK for iOS and this version actually added support for call kit it was a pretty interesting experience and we learned a bunch of things during this development cycle so one of the things we started with was that the RTC audio session that I mentioned before kind of has its own view of the world in its own set of state most this is based on what av audio session is doing itself however with call kit there's a little bit of a wrinkle into this to this problem and that's that if you go through the call kit API as Eric explains in the previous talk apples going to go ahead and activate the AV audio session for you so when they do this this is kind of without WebRTC C's knowledge and WebRTC kinks like at this point it hasn't actually activated the audio session but in reality apples went and done it through call kid so another thing we ran into was some sample rate mismatches and if this happens to you you're gonna figure it out really quickly because the audio is just gonna sound like garbage pretty much just like a computer talking to you or an old modem something like that it's not going to be good so what we discovered is there was a particular event that was not being handled internally by WebRTC this is this very long av audio session route configuration change notification and so we were able to actually just make a few lines of code change to actually get us to handle this event at which point we would always be in sync with the sample rate that was being produced by core audio versus what WebRTC actually expects another thing we ran into which is an issue that has been filed on the bug tracker was a risk for a projection so one thing that you know Apple was concerned about is like private API usage and we looked into this and what actually happened was that the RTC audio session just uses a selector that happens to conflict with something that apples got privately you know something we wouldn't have known because we don't have access to their source code but nevertheless this was causing some people to get their apps rejected so a simple change here by just renaming a couple of methods actually gets you past Apple's App Store review and gets you into the store so that's ticket 6382 and the chromium bug tracker if you're interested feel free to vote for it Google tends to prioritize tickets based on what you vote for so you know your stars count let them know what you think so moving with coke it one of the things that was pretty important that we had to do to get caulk it to work properly well this is a little bit of a hack on our side but this is the approach that we took for our first release supporting caulk it which was to use the manual audio option in RTC audio session so what we did was flip this on externally using this objective-c class RTC audio session and then from there we made a few internal changes just in order to satisfy the API contract that Apple has so in particular there's a couple things that WebRTC does they're completely automatic and it has behaviors like for example if you open up a peer connection it's going to start recording because it knows that you want to send audio to someone else now say you closed your last peer connection because you're no longer talking with anyone well WebRTC is gonna go automatically do a bunch of things it's going to unconfessed it's going to stop the audio unit that's causing playback and recording to happen and all this is gonna well this can sometimes get in the way of the API contract that Apple wants you to to adhere to for call kit so we ended up making a few modifications inside the audio device iOS implementation and that coupled with a somewhat hacky usage of use manual audio allowed us to actually get full call kid support and reliable usage of these api's another thing that was introduced in iOS 10 you may have noticed if you're kind of into AV audio session and you've you've used it a lot in the past is that they added a new way of a new audio threading mode so in this mode if you happen to have an iPhone 6s or a7 one of the ones that supports this live photos feature they actually changed audio in a certain way what they'll do by default if you don't tell them otherwise is they're gonna have multi-threaded audio capture and recording so they're gonna operate to real-time threads one for capture one for for playback and so what we did just because we didn't want to make any more changes to you know the source code for WebRTC is we would just force back to the old iOS 9 an earlier threading model and that you can do that by using the AV audio session io type aggregated and if you do that you'll get a single thread for real-time audio capture and this is going to get you back to sort of the eye ix behavior so we just did this for compatibility reasons we didn't want to have to have any more code past two tests and you know introduce more of a testing burden in our case so this is just something that we're doing for now hopefully we'll be able to you know kind of validate this behavior with the latest versions of WebRTC and you know kind of adopt a new way of thinking another thing that we that we discovered that's that's pretty interesting is av audio session modes so in particular WebRTC likes to use the voice chat mode and as you might imagine this modes actually a lot more useful for voice chat than it is for video chat so what we tend to do by default at this point and we actually give you a choice in our SDK is we use the video chat mode so this is pretty much the preferred mode if you were you know making audio and video calls so for example you know if I'm gonna call I'm typically holding up my phone and I'm using the front camera so if I'm doing that then the microphones I actually want to use are the ones on the top here that are pointed towards your face as you're speaking into the camera if you're in the voice chat mode what you're actually going to get is using the bottom array of microphones and this is actually not great if you're doing a video call because you're not speaking into those microphones and they're getting indirect sound and you're just not going to get very good voice intelligibility by doing this so my recommendation is that you should use the video chat mode if possible we have to make I believe it was only a few lines of code changes in WebRTC to support this there's there's a little bit of code that selects mono flyback and recording and it's keyed off of the voice chat mode so if you make that small change you'd be okay in this situation and you can take advantage of the video chat mode which is pretty much the preferred option on iOS if you're doing a video call so as I mentioned we just released a beta which has support for call kit over the past couple days we've also been talking to Google about potentially contributing some of the call kit changes that we made back to chromium WebRTC and that's hopefully our plan moving forward is to get at least some of these changes back into chromium so we don't have to keep pouring them forward no one wants that burden so we'd rather just put it upstream and let everyone take advantage of these improvements so that was a call kit for the next section of the talk I'm gonna move on to the video pipeline and I'm going to start with capture so first a definition of what captures and renders are captures you can think of them as sources which produce raw video but along with that they're also producing timing information and other metadata that's important when you're doing video capture render is on the other hand tend to consume raw video and usually the reason why you want to consume raw video is actually just a draw to screen but I'm sure you guys are all developers you can come up with some pretty cool things to do with renders besides just start of the screen nevertheless that's what they do by default moving on to codecs encoders are basically you know a module or a block that consumes raw video and on the other end produces an encoded bit stream that you can transmit over the network using RTP decoders on the other hand take encoded video consume it and on the other end they produce raw decoded video which you might send to a renderer in order to display it so on iOS there are three codecs to be aware of there they're pretty important and those are vp8 and vp9 also h.264 so there are a couple different approaches used to implementing these codecs on iOS Apple only provides hardware for h.264 so as you might imagine the vp8 and vp9 implementations are actually done in software so chromium WebRTC relies on a dependency called live VPX and this dependency includes support for both vp8 and vp9 now because these implementations are done in software there are a couple constraints on the system what you're gonna need to do is if you have you know captured buffers and you have memory coming out of your camera for example this isn't always in the same format that these software encoders expect so you're gonna have to take this these buffers that reside in hardware map memory and your need to do a format conversion and bring them into main memory now in the case of the WebRTC video pipeline it's expecting all of its software encoders to deal with i4 20 buffers so if you're not in that format you're gonna have to make a conversion in order to get to the format that they need for input on the h264 side we're able to take advantage of something called video toolbox and this is a framework that exists on both the Mac and iOS and what it allows you to do is take advantage of the hardware h.264 encoders that are built into your iOS device so this is an extremely popular feature I believe it started with WebRTC 45 this is when it first was introduced behind the flag and since then it's gotten 80 stars last time I checked on the bug tracker so this has been an extremely popular issue that everyone's voted for and Google's listened and done a really good job of improving this implementation over the past 10 releases of of chromium at this point as of 55 you're actually able to get something called zero copy and what I mean by that is that the buffers that are coming out of your camera or your video source are not copied on their way to the encoder so you can actually just pass just by doing reference counting you can pass these buffers without copying them all the way through the system and then feed them to your encoder so this is pretty cool another thing to note about the current implementation of h.264 on iOS is that it supports the baseline profile there is a ticket which you can take a look at which is about adding support for high profile on iOS there's also another ticket which is about negotiating high profile h.264 in your SD PS so feel free to take a look at those issues and vote for them if it's something you're interested in alright so that was a summary of the video pipeline we're going to take a look at what actually happens when you pass buffers through the pipeline using a software encoder so all this is going to start with a capture so in this case the capture on the far left here is going to be a camera capture and that capture is producing buffers that are in a format called env 12 and also those buffers are mapped to the hardware so they're there buffers that could be shared between every other piece of the system if you let them unfortunately in this case because we're using a vp8 encoder we're gonna have to do this this conversion into i4 20 and at the same time when we do this conversion we're also going to put those buffers into main system memory if you look at the rendering side this is actually looking pretty good and in WebRTC 55 if you just use that RTC eagle' video view which Eric talked in the last session what you'll get is your a copy rendering by default on iOS so that's pretty cool what it's gonna do is it's gonna pass those buffers into the renderer and the renderer can actually just seamlessly use them as OpenGL textures but in this case because they already exist and they're already mapped into Hardware you don't need to make another copy in order to actually use them in your renderer so the render is just going to run a simple shader which is going to do a format conversion for us it's gonna go from env 12 for the format we captured in and over to RGB a which we need to display to screen so let's contrast that with h.264 based encoding so you can see things have gotten a little bit nicer here the colors that you don't see any red that's good because that means we don't have to do any conversions so what's gonna happen here is it's pretty much the same path from the rendering side we're getting zero copy that's great but from the encoder side were able to seamlessly pass off of these buffers interested into the h.264 encoder at that point we're just gonna let video toolbox encode them without making another copy okay so we've now done we've talked about the video pipeline looked at a few basics of how it's set up on iOS so now let's let's talk about what is actually included in terms of captures that ship with WebRTC so if you've been using the objective-c api's to develop an iOS app whether it's written in Swift or objective-c you may have noticed RTC avfoundation video source this source allows you to use the camera it's pretty much as simple as that and it has an objective-c API and it lets you use the front and rear cameras so I mentioned just front and rear that means that you can't at this point use some of the new cameras that were added on the iPhone 7 so if you want to use like the telephoto lens for example you can't do that currently uses an older set of api's that work on iOS 9 and below so it's just not going to give you access to those other cameras or the fused camera that is both the wide and telephoto cameras on the iPhone 7 that being said this is still a pretty good capture it gives you a bunch of different capture formats that are supported out of the box so you can use what are called video constraints to select anywhere from about 360 P to 720p so you can kind of tailor this to your application whether you want high definition whether you want standard-definition it's up to you you can choose this and it's all supported out-of-the-box so internally this capture relies on something called avfoundation video capture and in this case this class is a c++ cricket video capture subclass and what it does is it pretty much acts as a bridge from the objective-c capture into the rest of the captured five line now you think you might be able to use this to write your own external captures and just take advantage of this C++ class in order to do everything you want but unfortunately it is specific to the requirements of using AV capture session and avfoundation so with that said let's talk a little bit about custom use cases for custom captures so why might you want to do this well I'm gonna give you a couple reasons the first one that's pretty important that a lot of people have asked about is screen sharing another one would be imagine you have a video game and it renders OpenGL or metal content and you want to actually share that content with someone else well in order to do that you might need to write or you would need to write your own custom capture another use case that could be important to you depending on your product is maybe you have video sitting locally on your computer or your iOS device and or or maybe you're streaming a video to your iOS device and you want to rebroadcast that using WebRTC well all these things are going to require you to write your own capture so later in this talk we're going to discuss how we would actually do that getting back to the camera use case one thing that you can do with the camera that is a little bit more advanced than what you're getting out of the box with with Google's implementation is you might want to do something like live image processing or you might want full manual controls so doing things like selecting this fused camera that has you know the wide and the telephoto lens on modern iPhones or you might want to do things like manual exposure zooming plenty of things like that that just aren't supported out of the box so if you write your own capture you could you could accomplish these things for your product so what we did at Twilio is we set out to build what I would call a generic video capture and this is something that well the actual name we called it was a core video capture and the reason for that is because this capture acts as a bridge between the objective-c world and C++ and what it takes is input is cor video buffers so this is basically taking the core video world of iOS and bridging that down into WebRTC alright so this core video capture it does pretty much everything you'd want from a WebRTC capture in that it's able to list formats it's able to provide metadata about the about the frames your your gonna push through the pipeline and also it's gonna support zero copy wherever possible alright so let's get into some coding we're gonna look at the API that we developed here for this capture it's pretty simple it only has four methods and what it allows you to do is list the the format's that you're going to support so this includes you know things like dimensions frame rates and pixel formats all these together comprise a video format and a list of many of these things describes what your capture actually supports in the field another property that we have in our generic capture is is screencasts so this this will indicate to the downstream pipeline that you're going to produce content that is either for the screen or not and the the video pipeline below is going to adjust accordingly so for screen content we're gonna want to focus on the clarity of the image and not frame rate and motion it also has methods to start and stop capture which we'll get into a little further in the next slide so when you start capture you're gonna be hen your going to be handed something called a video consumer this consumer allows you to deliver frames downwards to the pipeline it also allows you to signal when your capture is finished starting and this diagram helps to explain a little bit about what's going on here so imagine your capture which you have on your own thread it's going to start asynchronously WebRTC is going to come up on the worker thread and it's going to tell you that it's time to start capturing when this happens your capture can go off on its own thread and start asynchronously when it's done it'll signal that it started back to WebRTC and at this point the downstream pipeline knows everything it needs to know that you're ready to deliver frame and you've been able to start in the format that was requested from your capture and once your capture has started you're going to be producing frames hopefully on your own thread and you're gonna be able to deliver those frames over to the WebRTC worker thread where they can go through the pipeline to renderers to encoders and so on and so forth so next I'm gonna break things up a little bit with a live demo a screen capture is what we're gonna be building for the rest to talk so before we get into the code I'm just gonna show you what this thing actually looks like and how it works already so what I've done here is I've loaded up webrtc.org in my simulator let's just make that landscape so you can all see it so as I scroll down my capture is just capturing the video from the screen and it's just rendering it in the bottom corner there so you can see as I scroll I'm just capturing at a low frame rate at this point five frames a second and it's going ahead and just giving me a captured version of this screen that I can then share with others over a peer connection okay so that's the screen capture and let's get into how to build it so what this thing actually does is it shares the contents of a UI view it's going to do that by periodically drawing the view hierarchy and it'll use it what's called a CA display link timer in order to draw this at a very specific time interval we don't want to draw a particular frame more than once we only want to draw it you know a single time let's be efficient here so we're going to use this display link timer which is exactly synchronized to the vsync on your phone it's also getting you something called UI graphics image context in order to draw the view this is a pretty simple solution that doesn't require you to write a lot of code or manage a lot of your own buffers so we're just going to go with this for the purposes of an example so let's take a look at the drawing loop next the drawing loops pretty simple and for a screen capture it makes sense to operate on the main thread so we have a source which is a display link timer all the way on the far end of the screen here and that display link timer is operating on the main thread and it's going to call you back periodically when it's time to draw the screen so you're going to go ahead and draw your UI view at that point you're going to take what you get out of this drawing which is a UI image and you're going to convert that into a core video pixel buffer at which point you can signal that buffer over to WebRTC let it do its thing rendering and coding all that good stuff and you'll return control to the main run loop at which point it'll wait for the next display link timer to fire and you can draw your view again and repeat this whole process so the first thing we're going to do with this screen capture is describe what we actually support to WebRTC apologies if the if these slides are a little bit small on the back we've got a lot of code here but what we're gonna do is we're going to start by telling WebRTC if we're a screencast or not in this case it's a screen capture so obviously we are will return yes there next we're going to describe the formats that we support so in this case we only support a single format with this example capture so describing this to WebRTC is pretty easy so what we're gonna do is we're gonna rasterize at a 1x scale so as you might know if you've heard of like Retina screens things like that iOS devices can render it a lot of different scales for the case of our screen capture we're just gonna render out 1x which is kind of a simple you know not terribly CPU intensive way of drawing the screen in order to share it with other people and in our case how we're gonna render is a format called a RGB so we'll indicate that to WebRTC we're also going to know we're gonna target a frame rate of 5 frames a second rather than 60 or something higher again this is just to conserve CPU and because we're sharing with the screen with other people you don't really care as much about motion or at least for this particular example you don't need to so will indicate that we are capturing at 5 frames a second and because of the rasterization scale what we're gonna have is a dimensions equal to the screen where points are equal to pixels now we've done that we're going to move over to the drawing loop that I showed you earlier and we're going to actually draw the UI view so the way we'll do this is we're gonna just wrap everything in an auto release pool to clean up any temporary memory and then we're going to go ahead and use UI graphics image context to draw the contents of your view into a UI image after we've done that we'll just clean up the context grab the image and move on so the next step we have a UI image this is pretty good what we can get from the UI image is something called a CG image or a core graphics image so once we have that we can get access to the underlying data that represents the pixel buffer now in this particular example I mentioned we were using UI graphics context to draw so what we're gonna do here is we're actually gonna make a copy this is not great for performance reasons as I already explained earlier in the talk but it does simplify this example a little bit because we don't need to worry about providing our own buffers to draw into we're just gonna you know kind of use this built in system functionality grab an image out of it make a copy and then with that copy we're gonna create a cor video pixel buffer and pass this into WebRTC so the indications that we make here are just creating a pixel buffer with some existing bytes and we're going to pass this data that we copied from the UI image and now about data that we copied is going to have the same life cycle as the core video pixel buffer so at the time when that pixel buffers flowed all the way through the pipeline and is ready to be destroyed we're then going to destroy the underlying data that we copied all right so we've done pretty much all we need to do here and the final step would be delivering this buffer to WebRTC so we're just gonna package it in terms of the image data and the metadata and so that's pretty simple in our case we don't do anything fancy when we draw so our images are always going to be oriented upwards there's going to be no rotation tags so we'll just indicate essentially a 0 degree rotation so WebRTC knows that it does not need to rotate these buffers and it does not need to pass those tags on to the other parties over our peer connection next we're gonna go ahead and time stamp this this frame and what we're gonna do is use the display link timers time stamp as our capture time so we'll just take that time stamp convert it into units that make sense for WebRTC and then finally we're gonna go pass off this buffer into the WebRTC video pipeline now at this point we can relinquish control of the buffer and let WebRTC do its thing retaining the buffer is needed and what makes its way all the way through the pipeline will release it and it'll be free to go so with that said we've we've completed the screen capture built the demo that you saw and if you follow this link we have a couple things that are that are interesting the first one is a call kit example and this is available today just follow this link it's it's our video QuickStart for swift so this shows you how to use the call kit api's with our sdk and what we're going to be adding soon is a screen capture example similar to what you saw today but written in Swift so with that thank you very much you		